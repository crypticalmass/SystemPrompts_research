{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crypticalmass/SystemPrompts_research/blob/main/Grounding_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_lgX9omPXF-"
      },
      "source": [
        "## Gemini API: Getting started with information grounding for Gemini models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkR4fWudrHCs"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Grounding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKKNfXWrHgs"
      },
      "source": [
        "In this notebook you will learn how to use information grounding with [Gemini models](https://ai.google.dev/gemini-api/docs/models/).\n",
        "\n",
        "Information grounding is the process of connecting these models to specific, verifiable information sources to enhance the accuracy, relevance, and factual correctness of their responses. While LLMs are trained on vast amounts of data, this knowledge can be general, outdated, or lack specific context for particular tasks or domains. Grounding helps to bridge this gap by providing the LLM with access to curated, up-to-date information.\n",
        "\n",
        "Here you will experiment with:\n",
        "- Grounding information using <a href=\"#search_grounding\">Google Search grounding</a>\n",
        "- Adding <a href=\"#yt_links\">YouTube links</a> to gather context information to your prompt\n",
        "- Using <a href=\"#url_context\">URL context</a> to include website, pdf or image URLs as context to your prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKu1tRBrQ7xj"
      },
      "source": [
        "## Set up the SDK and the client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIWKUlPqP5NK"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "This guide uses the [`google-genai`](https://pypi.org/project/google-genai) Python SDK to connect to the Gemini models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fr84vJuPSHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a34385-5f00-4cdf-86b5-0cac081083af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.9/721.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Grounding with Google Maps was introduced in 1.43\n",
        "%pip install -q -U \"google-genai>=1.43.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a503bnWNQoCL"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjvgYmdLQd5s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhKXgMSNQrrV"
      },
      "source": [
        "### Select model and initialize SDK client\n",
        "\n",
        "Select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C75s1LR9QmOz"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "MODEL_ID = \"gemini-3-flash-preview\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.5-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abb962246f15"
      },
      "source": [
        "<a name=\"search_grounding\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mDMScex1It5"
      },
      "source": [
        "## Use Google Search grounding\n",
        "\n",
        "Google Search grounding is particularly useful for queries that require current information or external knowledge. Using Google Search, Gemini can access nearly real-time information and better responses.\n",
        "\n",
        "To enable Google Search, simply add the `google_search` tool in the `generate_content`'s `config` that way:\n",
        "```\n",
        "    config={\n",
        "      \"tools\": [\n",
        "        {\n",
        "          \"google_search\": {}\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHIcazUO0-xU",
        "outputId": "5749eacc-7ba4-4df2-c082-ab7676efaaa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Response**:\n The latest match of the Indian Premier League (IPL) was the **2025 Final**, which took place on **June 3, 2025**.\n\nThe **Royal Challengers Bengaluru (RCB)** won the match, defeating the **Punjab Kings (PBKS)** by **6 runs** to secure their maiden IPL title. \n\n### **Match Summary:**\n*   **Result:** Royal Challengers Bengaluru won by 6 runs.\n*   **Scores:**\n    *   **RCB:** 190/9 (20 overs)\n    *   **PBKS:** 184/7 (20 overs)\n*   **Venue:** Narendra Modi Stadium, Ahmedabad (rescheduled from Eden Gardens).\n*   **Key Performers:** RCB's victory was anchored by a strong total of 190, which they defended successfully despite a late charge from PBKS's Shashank Singh (61* off 30 balls). Bowling highlights for RCB included Krunal Pandya and Bhuvneshwar Kumar, who both took two wickets each.\n*   **Significance:** This victory marked RCB's first-ever IPL trophy in the tournament's 18-year history.\n\nPrior to the 2025 season, the **2024 IPL Final** was won by the **Kolkata Knight Riders (KKR)**, who defeated the Sunrisers Hyderabad (SRH) by 8 wickets on May 26, 2024. As of February 2026, the 2026 IPL season has not yet begun, as the tournament typically commences in late March or April."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Query: ['latest IPL match result 2024 winner', 'who won the last IPL match']\n",
            "Search Pages: jagranjosh.com, olympics.com, wikipedia.org, youtube.com, wikipedia.org\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGB8ZWKql54v1VdVyJx5nzyrFskIGb01jPcOzYExSlj8PwF5hzGM80zs7WgtJGYStRHSdf5Nj0hFdLLbRR7Z77PfRahA39GkFrr4uGuIBJNp0nf8j8imwikzLYjesErBpcHODgjfhSdGEWWqKIeQULtDbqv2Ox0UViBAccgt3hw9FZB4vTKyZXiTbEO_zUpv4WekLMwzTqeRNS-hL9c\">who won the last IPL match</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSE4TouVsvRvC8W4FNc7bkpeRx7mIab1U_CHvqAcxgb8qRolMIyF4d0rKFZOxhmlh-o42Cct6VZDwDh0Oenlo3l6sm2Rc_uoiZCR6ttGAUKkhOAhIBm_eyuscPFu2aaZDRfgUHZ_uKlqx9PoHbjR6VnNcqQSxUy8_G5r6Qsf_SKk00IkKHyexnnUVYYeNYPyAaqfo72jpB-8dqgPBlz_mecNZYFFS2\">latest IPL match result 2024 winner</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What was the latest Indian Premier League match and who won?\",\n",
        "    config={\"tools\": [{\"google_search\": {}}]},\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(f\"**Response**:\\n {response.text}\"))\n",
        "# print the search details\n",
        "print(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n",
        "# urls used for grounding\n",
        "print(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n",
        "\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wROLHEYLLBHX"
      },
      "source": [
        "You can see that running the same prompt without search grounding gives you outdated information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdUkQ40cKaGX",
        "outputId": "16a0fa74-b6c8-466c-b886-eec041621e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The latest match in the Indian Premier League (IPL 2024) was **Qualifier 2**, which took place on May 24, 2024.\n\n**Sunrisers Hyderabad (SRH)** won the match, defeating the Rajasthan Royals (RR) by **36 runs**.\n\n**Match Summary:**\n*   **SRH Score:** 175/9 (20 overs)\n*   **RR Score:** 139/7 (20 overs)\n*   **Venue:** MA Chidambaram Stadium, Chennai\n*   **Key Performers:** Heinrich Klaasen scored a vital 50 for SRH, while Shahbaz Ahmed (3/23) and Abhishek Sharma (2/24) dominated with the ball to restrict Rajasthan.\n\nWith this win, **Sunrisers Hyderabad** have qualified for the Final, where they will face the **Kolkata Knight Riders (KKR)** on Sunday, May 26."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What was the latest Indian Premier League match and who won?\",\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE6Ft1wxSxO_"
      },
      "source": [
        "For more examples, please refer to the [dedicated notebook ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](./Search_Grounding.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6f7711ac06"
      },
      "source": [
        "<a name=\"maps_grounding\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bef61bf2764f"
      },
      "source": [
        "<a name=\"yt_links\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XfNrFR7j6F6"
      },
      "source": [
        "## Grounding with YouTube links\n",
        "\n",
        "You can directly include a public YouTube URL in your prompt. The Gemini models will then process the video content to perform tasks like summarization and answering questions about the content.\n",
        "\n",
        "This capability leverages Gemini's multimodal understanding, allowing it to analyze and interpret video data alongside any text prompts provided.\n",
        "\n",
        "You do need to explicitly declare the video URL you want the model to process as part of the contents of the request using a `FileData` part. Here a simple interaction where you ask the model to summarize a YouTube video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akVTribOkgT2",
        "outputId": "01b4c210-e8cc-4030-893d-92d43257a877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ju-yeong Ji, from Google DeepMind's AI Developer Relations, presents Gemma’s capabilities in the world of chess. He emphasizes that while AI chess engines like AlphaZero excel at move calculation, Gemma offers a new dimension through its language-processing strengths.\n\nGemma's key features in chess include:\n\n*   **Move Analysis:** It can translate technical chess engine outputs into easy-to-understand explanations, detailing why a move is strategic, what its goals are, and what potential risks are involved.\n*   **Storytelling:** Gemma can narrate chess matches by incorporating context like players, tournaments, and historical significance, making games more engaging than just a series of moves.\n*   **Educational Support:** It serves as a personalized tutor, explaining chess concepts and strategies in multiple languages and adjusting its level to the player's expertise.\n*   **Integration with Analytical Engines:** Gemma can be combined with chess engines to provide both the best possible moves and clear, human-readable explanations of the underlying strategy.\n\nOverall, Gemma aims to make chess more accessible and engaging by bridging the gap between technical analysis and human understanding."
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "yt_link = \"https://www.youtube.com/watch?v=XV1kOFo1C8M\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Summarize this video.\"),\n",
        "            types.Part(file_data=types.FileData(file_uri=yt_link)),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR7sQVlxy8Yr"
      },
      "source": [
        "But you can also use the link as the source of truth for your request. In this example, you will first ask how Gemma models can help on chess games:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTH4DqBAzx3H",
        "outputId": "e471a78d-150b-4e57-b28e-bcda405a6fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Gemma models significantly enhance the chess experience by bridging the gap between technical analysis and human understanding. While traditional chess engines excel at calculating optimal moves, Gemma can interpret these results and explain the strategic reasoning behind them in plain language. It can analyze complex game sequences to pinpoint key tactical moments and articulate the \"why\" behind a particular move, such as controlling the center or setting up a future attack. Furthermore, Gemma can transform chess games into compelling narratives, weaving together moves, player backgrounds, and tournament contexts to bring the history and drama of the game to life.\n\nAs an educational tool, Gemma acts as a personalized chess tutor available 24/7 in multiple languages. It can explain fundamental and advanced concepts, from opening theories like the Sicilian Defense to specific tactical ideas like passed pawns, tailoring its explanations to the user’s skill level. By integrating with analytical engines, Gemma provides a comprehensive learning experience—it can retrieve the best possible move and then generate an intuitive explanation of its advantages. This combination of deep calculation and linguistic capability makes chess more accessible and engaging for beginners and advanced players alike."
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "yt_link = \"https://www.youtube.com/watch?v=XV1kOFo1C8M\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                text=\"In 2 paragraph, how Gemma models can help on chess games?\"\n",
        "            ),\n",
        "            types.Part(file_data=types.FileData(file_uri=yt_link)),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHhdfKqLz_D6"
      },
      "source": [
        "Now your answer is more insightful for the topic you want, using the knowledge shared on the video and not necessarily available on the model knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2de8c7349137"
      },
      "source": [
        "<a name=\"url_context\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBPhxA-0RiT"
      },
      "source": [
        "## Grounding information using URL context\n",
        "\n",
        "The URL Context tool empowers Gemini models to directly access and process content from specific web page URLs you provide within your API requests. This is incredibly interesting because it allows your applications to dynamically interact with live web information without needing you to manually pre-process and feed that content to the model.\n",
        "\n",
        "URL Context is effective because it allows the models to base its responses and analysis directly on the content of the designated web pages. Instead of relying solely on its general training data or broad web searches (which are also valuable grounding tools), URL Context anchors the model's understanding to the specific information present at those URLs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7GrocBgYgrp"
      },
      "source": [
        "### Process website URLs\n",
        "\n",
        "If you want Gemini to specifically ground its answers thanks to the content of a specific website, just add the urls in your prompt and enable the tool by adding it to your config:\n",
        "```\n",
        "config = {\n",
        "  \"tools\": [\n",
        "    {\n",
        "      \"url_context\": {}\n",
        "    }\n",
        "  ],\n",
        "}\n",
        "```\n",
        "\n",
        "You can add up to 20 links in your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOXM1Fh2D9Ai",
        "outputId": "03ff701c-8c95-4ae7-e4c6-1829f670d8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the current documentation at [ai.google.dev/gemini-api/docs/models](https://ai.google.dev/gemini-api/docs/models) (as of February 2026), the Gemini model lineup has evolved through several generations, focusing on intelligence, reasoning, and efficiency.\n\nThe following table summarizes the key differences between Gemini 1.5, 2.0, 2.5, and 3.\n\n### Comparison Table: Gemini Model Generations\n\n| Feature | Gemini 1.5 (Legacy) | Gemini 2.0 (Deprecated) | Gemini 2.5 (Current Performance) | Gemini 3 (Frontier Intelligence) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Core Value** | Original long-context workhorse. | Next-gen features & native tool use. | Best price-performance & advanced thinking. | Most intelligent, agentic, & vibe-coding model. |\n| **Top Model** | Pro 1.5 | Flash 2.0 | Pro 2.5 | Pro 3 |\n| **Max Context Window** | 1M - 2M tokens | 1M tokens | 1M tokens | 1M+ tokens |\n| **Max Output Tokens** | ~8K tokens | 8,192 tokens | 65,536 tokens | 65,536 tokens |\n| **Thinking Support** | Limited / Basic | Experimental | **Full Support** (Pro & Flash) | **Optimized** (State-of-the-art) |\n| **Capabilities** | Multimodal (Text, Image, Audio, Video) | Native Tool Use, Superior Speed | Agentic workflows, High-volume processing | Richer visuals, Deeper interactivity, Frontier reasoning |\n| **Status** | Legacy / Retired | Deprecated (Shutdown Mar 2026) | **Stable** / Production Ready | **Preview** / Latest Intelligence |\n\n---\n\n### Key Differences Explained\n\n1.  **Gemini 3 (Frontier Intelligence):**\n    *   **Agentic Power:** Specifically designed for complex agentic workflows and \"vibe-coding\" (highly interactive, intuitive coding and development).\n    *   **Intelligence:** Currently ranked as the most intelligent model for multimodal understanding and reasoning.\n    *   **Enhanced Output:** Offers a massive increase in output token limits (up to 64K+) compared to earlier generations, allowing for much longer generated responses.\n\n2.  **Gemini 2.5 (High Performance & Reasoning):**\n    *   **Price-Performance:** The \"Flash\" version is optimized for large-scale processing and low latency while maintaining \"Thinking\" capabilities.\n    *   **Thinking Model:** Gemini 2.5 Pro is the standard for complex reasoning in code, math, and STEM, effectively replacing the niche formerly held by 1.5 Pro.\n    *   **Native Audio:** Includes specialized versions (like Flash-Native-Audio) for real-time audio generation and understanding via the Live API.\n\n3.  **Gemini 2.0 (Second Generation - Deprecated):**\n    *   **Transitionary:** Introduced native tool use and improved speed over the 1.0/1.5 series.\n    *   **Legacy Limits:** Limited by lower output token caps (8K) and experimental \"Thinking\" features compared to the robust implementation in 2.5 and 3.\n\n4.  **Gemini 1.5 (The Context Pioneer):**\n    *   **Legacy Status:** While it pioneered the 1M+ token context window, it is now considered legacy. Its reasoning and speed have been surpassed by the Gemini 2.5 Flash and Pro models, which offer similar context windows with significantly better intelligence and efficiency."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Based on https://ai.google.dev/gemini-api/docs/models, what are the key\n",
        "  differences between Gemini 1.5, Gemini 2.0, Gemini 2.5 and Gemini 3 models?\n",
        "  Create a markdown table comparing the differences.\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt], model=MODEL_ID, config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPCD5f2MSIx"
      },
      "source": [
        "You can see the status of the retrival using `url_context_metadata`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5kKeAX5MUsP",
        "outputId": "356cc246-e834-47c1-e1e5-b96259fbdfa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url_metadata=[UrlMetadata(\n",
            "  retrieved_url='https://ai.google.dev/gemini-api/docs/models',\n",
            "  url_retrieval_status=<UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: 'URL_RETRIEVAL_STATUS_SUCCESS'>\n",
            "), UrlMetadata(\n",
            "  retrieved_url='https://ai.google.dev/gemini-api/docs/models',\n",
            "  url_retrieval_status=<UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: 'URL_RETRIEVAL_STATUS_SUCCESS'>\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "# get URLs retrieved for context\n",
        "print(response.candidates[0].url_context_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS2xyoMPY8u-"
      },
      "source": [
        "### Add PDFs by URL\n",
        "\n",
        "Gemini can also process PDFs from an URL. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeMZX5C5sLe3",
        "outputId": "0a817016-b9ce-4f0d-c026-6cc8047197c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:15: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-3794376347.py:15: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  display(Markdown(response.text.replace(\"$\", \"\\$\")))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Alphabet's Q2 2025 earnings release reports significant financial growth, driven primarily by strong performance in Google Search and the expanding Google Cloud division. The company recorded substantial year-over-year increases in both total revenue and net income, reflecting robust advertiser demand and AI-driven product enhancements. Leadership highlighted ongoing investments in artificial intelligence infrastructure as a key factor in improving user experience and operational efficiency. Additionally, the report outlines capital allocation strategies, including share repurchases and the continued scaling of YouTube’s subscription services."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Can you give me an overview of the content of this pdf?\n",
        "  https://abc.xyz/assets/cc/27/3ada14014efbadd7a58472f1f3f4/2025q2-alphabet-earnings-release.pdf\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt], model=MODEL_ID, config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text.replace(\"$\", \"\\$\")))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  Can you explain Figure 1 in this PDF file\n",
        "  https://arxiv.org/pdf/1706.03762\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt], model=MODEL_ID, config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text.replace(\"$\", \"\\$\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "Dwz0L-H3eZCw",
        "outputId": "5f21d644-a664-4112-d0d4-97211790e583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:15: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-4075067263.py:15: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  display(Markdown(response.text.replace(\"$\", \"\\$\")))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Figure 1 in the paper **\"Attention Is All You Need\"** represents the **Transformer model architecture**. It is the blueprint for the modern AI models we use today (like GPT-4 and Claude).\n\nThe diagram is split into two main sections: the **Encoder** (on the left) and the **Decoder** (on the right). Here is a detailed breakdown of what is happening in each part:\n\n### 1. The Encoder (Left Side)\nThe job of the encoder is to process the input text (like an English sentence) and turn it into a mathematical representation that captures the meaning and context of every word.\n*   **Input Embedding & Positional Encoding:** Words are converted into numbers (embeddings). Since Transformers don't process words one-by-one (like older RNNs), they add \"Positional Encodings\" to give the model a sense of where each word is located in the sentence.\n*   **Multi-Head Attention:** This is the \"secret sauce.\" It allows the model to look at every other word in the sentence simultaneously to understand context. For example, in the sentence *\"The bank of the river,\"* the word \"bank\" attends to \"river\" to know it's not a financial institution.\n*   **Feed Forward:** A simple neural network that processes the information further.\n*   **Nx Block:** The \"Nx\" indicates that this entire block is stacked 6 times to create a deep, complex understanding of the text.\n\n### 2. The Decoder (Right Side)\nThe decoder’s job is to take the encoder's representation and generate the output (like a French translation), one word at a time.\n*   **Masked Multi-Head Attention:** Similar to the encoder, but \"masked\" so that when the model is predicting the next word, it can't \"cheat\" by looking at the words that come after it.\n*   **Encoder-Decoder Attention:** This is a crucial bridge. It allows the decoder to \"look back\" at the original input sentence (from the encoder) to make sure the translation stays accurate to the source.\n*   **Linear & Softmax:** At the very top, the model turns its internal math back into a probability distribution over its entire vocabulary to pick the most likely next word.\n\n### 3. Key Symbols in the Diagram\n*   **Add & Norm:** These are \"Residual Connections\" followed by \"Layer Normalization.\" They are technical tricks that allow the model to be very deep without the math \"breaking\" or becoming unstable during training.\n*   **Arrows:** The arrows show the flow of data. Notice the arrow going from the top of the Encoder into the middle of the Decoder—this represents the decoder \"paying attention\" to the input sentence while it generates the output.\n\n### Summary\nIn short, Figure 1 shows a system that **reads** a whole sentence at once (Encoder), **remembers** the context of every word (Self-Attention), and then **writes** a response (Decoder) by constantly referring back to what it read."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAkMWXwAxiaT"
      },
      "source": [
        "### Add images by URL\n",
        "\n",
        "Gemini can also process images from an URL. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPNxQYkx8WJN",
        "outputId": "92d34d3f-17f4-4d83-9368-a43309bc9e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Voici les noms des différentes parties du trombone numérotées sur l'image, en français :\n\n1. **Coulisse d'accord** (permet d'ajuster la justesse générale de l'instrument).\n2. **Embouchure** (la pièce où le musicien pose ses lèvres).\n3. **Pavillon** (la partie évasée d'où sort le son).\n4. **Clé d'eau** (ou clé de purge, pour évacuer la condensation).\n5. **Coulisse (mobile)** (la partie que l'on fait glisser pour changer les notes).\n6. **Entretoise de la coulisse** (la barre transversale que le musicien tient pour manipuler la coulisse).\n7. **Entretoise du pavillon** (barre de renfort sur la section du pavillon).\n8. **Verrou de la coulisse** (petite bague qui permet de bloquer la coulisse lorsqu'on ne joue pas)."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Can you help me name of the numbered parts of that instrument, in French?\n",
        "  https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Trombone.svg/960px-Trombone.svg.png\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt], model=MODEL_ID, config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHs8FDfSsjt2"
      },
      "source": [
        "## Mix Search grounding and URL context\n",
        "\n",
        "The different tools can also be use in conjunction by adding them both to the config. It's a good way to steer Gemini in the right direction and then let it do its magic using search grounding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEOpBpbssjbD",
        "outputId": "31407222-6b84-4128-d594-f0e28301badb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:20: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-110788399.py:20: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  display(Markdown(response.text.replace('$','\\$')))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Alphabet Q2 2025 earnings release (dated July 23, 2025) depicts a company in a state of \"accelerated transformation,\" balancing robust core growth with massive, front-loaded investments in artificial intelligence infrastructure.\n\n### **Overview of the Earnings Release Content**\n\nThe report highlights a \"double beat\" on both the top and bottom lines, driven primarily by a surge in Google Cloud and the resilience of Search advertising.\n\n*   **Top-Line Financials:** \n    *   **Revenue:** \\$96.43 billion, a **14% increase** year-over-year (YoY).\n    *   **Net Income:** \\$28.2 billion, up **19% YoY**.\n    *   **EPS (Earnings Per Share):** \\$2.31, significantly beating the analyst consensus of ~\\$2.17.\n*   **Segment Performance:**\n    *   **Google Search & Other:** Delivered double-digit revenue growth (\\$71.34B), dismissing fears that AI-led search competition would immediately erode its core business.\n    *   **Google Cloud:** A standout performer with **32% growth** (reaching \\$13.6B). Notably, its annual revenue run-rate surpassed **\\$50 billion**, achieving record profitability as enterprise adoption of Gemini and AI infrastructure scaled.\n    *   **YouTube:** Continued growth in YouTube ads and subscriptions, with management noting that **YouTube Shorts** now generates revenue per watch-hour comparable to traditional instream ads in the U.S.\n*   **Strategic & Capital Allocation:**\n    *   **CapEx Surge:** Alphabet raised its total 2025 capital expenditure outlook to approximately **\\$85 billion** (up from previous estimates of ~\\$75B), focused almost entirely on AI servers and data centers.\n    *   **AI Integration:** The release emphasized the rollout of **Gemini 2.5** and the success of **AI Overviews**, which now serves over 2 billion monthly users.\n\n---\n\n### **Financial Analyst Reaction & Market Trends**\n\nThe overall trend among Wall Street analysts is **\"bullish but cautious regarding the ROI of spending.\"** While the operational results were stellar, the massive increase in CapEx sparked a debate over when these investments will translate into bottom-line returns.\n\n#### **1. Main Analyst Reactions**\n*   **Goldman Sachs:** Raised its price target to **\\$234** (from \\$225), citing the \"accelerating business fundamentals\" and Alphabet’s leadership in AI and cloud computing. They view the high CapEx as a necessary move to secure long-term dominance.\n*   **BMO Capital:** Reiterated an **Outperform** rating with a **\\$200** target, highlighting that Google’s AI advancements (like Rich Communication Services and Gemini) are successfully defending its ad-revenue moat.\n*   **Morgan Stanley:** Maintained a target of **\\$185**, focusing on the \"improvement in ad relevance\" and the potential of Cloud to become a primary earnings driver alongside Search.\n*   **Seeking Alpha / Independent Analysts:** Noted a slight drop in Google's total search market share (down to 89% from 92% per StatCounter) due to ChatGPT and Perplexity, but characterized the company as \"quietly dominating\" because the total market for search is still expanding.\n\n#### **2. Key Trends Identified**\n*   **The \"Cloud as the Second Engine\":** Analysts have shifted their view of Alphabet from a one-trick pony (Ads) to a dual-engine company. The 32% growth in Cloud is seen as a signal that Alphabet is successfully capturing the enterprise AI \"arms race.\"\n*   **Search Resilience:** The \"bear thesis\" that AI chatbots would kill Google Search has largely been tabled for now. Analysts noted that \"AI Overviews\" actually appear to be increasing user engagement rather than cannibalizing clicks.\n*   **Efficiency vs. Investment:** There is a growing trend of analysts watching the \"Other Income\" and \"Operating Margins\" (32.4%) closely. While Alphabet is growing, the cost of talent and hardware is rising, leading to a \"show-me\" story for 2026 regarding the actual profitability of these AI services.\n\n**Summary Trend:** The stock is currently viewed as a **\"Core AI Holding.\"** The market has accepted high spending as a prerequisite for survival, rewarding Alphabet for its ability to maintain high margins while rebuilding its entire tech stack for the AI era."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE5lzXaHrnYoM2fy1ZxBOUgVIWbMN-HbaxeSOMi1PxTXKHQmIEXVE-3rKpRH-2oRWAvMTBKeTTX9R79Jz1icf9807n_LBZaZNtvt9UYXAJs-32RwHwrvmPEq9FnidDxx5RWq4Brw8nKW5JPCw9CL-g8lyGiTdNghNRTqArB8O3KSHtSIPZAz-qK7mzxdw-sSZMz_0QL8zC8Uw6gbSx4546Bz6VyxbFXv0qQCDmZmy1vo8HEWNHVBy15ogI3CLENww==\">financial analysts reaction Alphabet Q2 2025 earnings trend</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGNamQjBe08oaiXqrqRfVlzhFtV1nQTUiH74skXGqpSyW6nZt6_FE6i5qI6bKgb_1Xg86Zm1yNr9SjYCrX0zQOGPpMuKUUBNRGvkW4185Ev8YGWOaTJRcGZbavR9nBoSmJ-kf3RHFAiGSwxR7Tx1j_yRtURHGDTHMeP1lMyjFYXYC9MiZHb7lZDaPQygFiuagXbtGa5cZjD62BmjqBMEgOPk7mXw9bEc5zFDwVxIfBQ9w3UAOs=\">Alphabet Q2 2025 earnings release results summary</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG6FDZ-SjNHZSsOGa66LWVY5Dgns521gYp_foMEIA6yEOnoM-PvOnDvHFzqxqyLtLhQDWB8GeiDj6Rx1KkLqubzgq1mDJWbz5X5_lhUL2IKof_4GDvX1bbTWHELHCc5oWIvKWB0YGA1VgAVUyqpsJmmSzbiWML9FeyfiwrDTyuZPQkbH3R7cf9A1MkpmzcJjBb2Kuyrxmc0VjcYCC3zV3SKnnpzvAe6ARtjpIdXhaIjStEFdEQYCIdnpF_rkOPSpFg=\">Alphabet Q2 2025 financial highlights revenue net income EPS</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH-B3EoteE5FxNrb7Kp_FdQCOSA301epUU6UxTYwA_FP2fHgvdCgMG0OTp3kodCiH2mBRNIhzqYs4A4lvVx6gx941syjnc7H3UxJjvbspUhoOyz6baownqlLNP2-XRJMbBEGGtprOBYvSyin4rfBUt6yu4BJwLgpkLlmNOtHUvMqjFiuryLrtSFYR2U7JPF2zpyHj4d0lQPkzf8J_ZzZVVUUAnPTS2AduBe3iZWAhf3l2E=\">Alphabet Q2 2025 earnings report key takeaways</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Can you give me an overview of the content of this pdf?\n",
        "  https://abc.xyz/assets/cc/27/3ada14014efbadd7a58472f1f3f4/2025q2-alphabet-earnings-release.pdf\n",
        "  Search on the web for the reaction of the main financial analysts, what's the trend?\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "  \"tools\": [\n",
        "      {\"url_context\": {}},\n",
        "      {\"google_search\": {}}\n",
        "  ],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  contents=[prompt],\n",
        "  model=MODEL_ID,\n",
        "  config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text.replace('$','\\$')))\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMjU56RidcVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}